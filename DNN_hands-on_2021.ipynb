{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2-p_EZYaY02"
   },
   "source": [
    "# Lab 1: Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_S63SmkaY1q"
   },
   "source": [
    "# 1.1 Keras basics\n",
    "<img src=\"https://drive.google.com/uc?id=1qUd5pI-C3DOFbJs8tGos9OCKZmPnNZYB\" width=\"400px\"><br>\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow.<br>\n",
    "It allows for easy and fast prototyping and supports both convolutional networks and recurrent networks.<br>\n",
    "Its most important features are: user friendliness, modularity, easy extensibility\n",
    "\n",
    "In this section we will:\n",
    "* 1.1.1 Build a Multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HuNs_LWFaY1x"
   },
   "source": [
    "#### Build the same Single Layer Perceptron with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGyGeFxcaY1y"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the number of inputs and outputs\n",
    "n_input_nodes = 2\n",
    "n_output_nodes = 2\n",
    "\n",
    "# First define the model \n",
    "model = Sequential() # model lets us define a linear stack of network layers.\n",
    "\n",
    "# define our single fully connected network layer\n",
    "dense_layer = Dense(n_output_nodes, activation='sigmoid',kernel_initializer=\"Ones\",bias_initializer=\"Ones\")\n",
    "\n",
    "# Add the dense layer to the model\n",
    "model.add(dense_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7O3ScCB-aY19"
   },
   "source": [
    "## 1.1.2 Build a Multilayer perceptron\n",
    "Let's build a multilayer perceptron; MLPs are fully connected, each node in one layer connects with a certain weight to every node in the following layer.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=16Uxs11_l8xl58978mRsxgEtdaTSG5KGb\" width=\"400px\"><br>\n",
    "\n",
    "Try to build one composed by two hidden dense layer with ReLU activation and one dense output layer(units=1) with sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8PMtoQ8aY2A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Generate dummy data\n",
    "mu1=1\n",
    "sigma1=0.1\n",
    "mu2=0\n",
    "sigma2=2\n",
    "train_data = np.zeros((2000))\n",
    "\n",
    "train_data[0:1000] = np.random.normal(mu1,sigma1,1000)\n",
    "train_data[1000:2000] = np.random.normal(mu2,sigma2,1000)\n",
    "\n",
    "# train_data[0:1000] = np.random.uniform(-10,10, size=(1000,))\n",
    "# train_data[1000:2000] = np.random.uniform(20,50, size=(1000,))\n",
    "\n",
    "train_labels = np.zeros((2000,2))\n",
    "train_labels[1000:2000,1] = 1\n",
    "train_labels[0:1000,0] = 1\n",
    "test_data = np.zeros((200))\n",
    "\n",
    "test_data[0:100] = np.random.normal(mu1,sigma1,100)\n",
    "test_data[100:200] = np.random.normal(mu2,sigma2,100)\n",
    "\n",
    "# test_data[0:100] = np.random.uniform(-10,10, size=(100,))\n",
    "# test_data[100:200] = np.random.uniform(20,50, size=(100,))\n",
    "\n",
    "\n",
    "test_labels = np.zeros((200,2))\n",
    "test_labels[0:100,0] = 1\n",
    "test_labels[100:200,1] = 1\n",
    "\n",
    "# Build your first model by creating a Sequential object and then adding 3 Dense layers:\n",
    "units = 32\n",
    "# Create a Sequential\n",
    "model = Sequential()\n",
    "# Add a Dense layer with 32 neurons, with relu as activation function and input dimension equal to the number of features\n",
    "model.add(Dense(units, activation='relu'))\n",
    "# Add a Dense layer with 32 neurons, with relu as activation function\n",
    "model.add(Dense(units, activation='relu'))\n",
    "# To produce the output Add a Dense layer with 1 neurons, with sigmoid as activation function\n",
    "model.add(Dense(2, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4416,
     "status": "ok",
     "timestamp": 1588274136297,
     "user": {
      "displayName": "Nicoletta Noceti",
      "photoUrl": "",
      "userId": "00427744574778931345"
     },
     "user_tz": -120
    },
    "id": "b0k0N6xrgVCc",
    "outputId": "038d1390-e6e0-4a47-a067-4716743f15de"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "# The fit function output is a History object. The history.history attribute is a record of\n",
    "# training loss values and metrics values at successive epochs, as well as validation loss values \n",
    "# and validation metrics values \n",
    "history = model.fit(train_data, train_labels, epochs=30, batch_size=128)\n",
    "_, train_acc = model.evaluate(train_data, train_labels, verbose=1)\n",
    "_, test_acc = model.evaluate(test_data, test_labels, verbose=1)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to use linear activation function. (activation='linear')**. \n",
    "\n",
    "How do the performances change? why?\n",
    "\n",
    "**Try the same model with linear activation function on samples extracted from two uniform distribution** \n",
    "\n",
    "Uncomment the lines with np.random.uniform and comment the lines with np.random.normal in the previous cell code. \n",
    "\n",
    "**Try to use non-linear activation function on this toy dataset.** Do the results improve? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8fIyluGaY2E"
   },
   "source": [
    "# 1.2 Build a Deep Neural Network \n",
    "In this section we will:\n",
    "*  1.2.1 Import the dataset\n",
    "*  1.2.2 Build a model\n",
    "*  1.2.3 Train the model \n",
    "*  1.2.4 Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vnq7w29FaY2G"
   },
   "source": [
    "## 1.2.1 Import the Dataset\n",
    "Fashion-MNIST is a dataset of Zalando’s article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. <br>\n",
    "Each example is a 28×28 grayscale image, associated with a label from 10 classes.<br>\n",
    "<img src=\"https://drive.google.com/uc?id=1bCzydSawhVLUx9F3ee18eIwy0U3eNRMZ\" width=\"600px\"><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3550,
     "status": "ok",
     "timestamp": 1588274146782,
     "user": {
      "displayName": "Nicoletta Noceti",
      "photoUrl": "",
      "userId": "00427744574778931345"
     },
     "user_tz": -120
    },
    "id": "LX4Gc3siaY2H",
    "outputId": "e43aadc4-c18a-4c09-c108-e51b74d96663"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "mnist_fashion = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_learn, y_learn),(x_test, y_test) = mnist_fashion.load_data()\n",
    "x_learn, x_test = x_learn / 255.0, x_test / 255.0 # normalization \n",
    "x_train, x_val, y_train, y_val = train_test_split(x_learn, y_learn, test_size=0.3, random_state=42) # split learn in train,val\n",
    "num_classes = 10 # Fashion-MNIST classes\n",
    "\n",
    "print(x_train.shape, x_val.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_LsI1CdaY2J"
   },
   "source": [
    "#### Plot some sample from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1776,
     "status": "ok",
     "timestamp": 1588274152252,
     "user": {
      "displayName": "Nicoletta Noceti",
      "photoUrl": "",
      "userId": "00427744574778931345"
     },
     "user_tz": -120
    },
    "id": "gX44XG1RaY2K",
    "outputId": "3beef4c1-79cc-473f-c2e7-54c2e2342647"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7Ag6933aY2N"
   },
   "source": [
    "## 1.2.2 Build a Model\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1siIVKXLL7_YsecqlCQqw_ZI35LhR8XbI\" width=\"400px\"><br>\n",
    "\n",
    "A Deep Neural Network is a neural network composed by many layers and consequently it has a deeper structure. The number of layers in the network depends on different factors: for example on the data available, on the complexity of the problem, on the computational power and so on.\n",
    "The value produced as output by a neuron is determined by the input the neuron receives and by the activation function. There exists different choices for the activation function. One of the most used is Relu but it depends on the data and on the network architecture.\n",
    "<br><br>\n",
    "<img src=\"https://drive.google.com/uc?id=1Rr_1OJeORL6qmGFplC0E2hFj-KapFGN0\" width=\"600px\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUrCyhLdaY2R"
   },
   "source": [
    "#### Build a model with this structure: Flatten+Dense(ReLU)+Dense(ReLU)+Dense(ReLU)+Dense(ReLU)+Dense(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jL-UXDSMaY2S"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "# https://keras.io/layers/core/\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "# Add a Dense layer with 512 neurons, with relu as activation function\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# Add a Dense layer with 256 neurons, with relu as activation function\n",
    "model.add(Dense(256, activation='relu'))\n",
    "# Add a Dense layer with 128 neurons, with relu as activation function\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Add a Dense layer with 64 neurons, with relu as activation function\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# Add a Dense layer with number of neurons equal to the number of classes, with softmax as activation function\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXOzjck_aY2V"
   },
   "source": [
    "When we are building a model there are many design choises that we must operate: the choice of a Loss Function, the Metrics and the Optimizer.<br>\n",
    "\n",
    "**Loss functions** are used to compare the network's predicted output  with the real output, in each pass of the backpropagations algorithm<br>\n",
    "Common loss functions are: mean-squared error, cross-entropy, and so on...<br><br>\n",
    "**Metrics** are used to evaluate a model; common metrics are precision, recall, accuracy, auc,..<br>\n",
    "\n",
    "The **Optimizer** determines the update rules of the weights. The performance and update speed may heavily vary from optimizer to optimizer; in choosing an optimizer what's important to consider is the network depth, the type of layers and the type of data.<br>\n",
    "The gifs below give an idea on how different Optimizers work.<br>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1UfM-0QruDiOgy8cSxLmZHx3FI4pzLeDQ\" width=\"360px\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01Ms84EmaY2W"
   },
   "source": [
    "#### Configures the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zL2LLmKyaY2W"
   },
   "outputs": [],
   "source": [
    "# Optimizers    https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Adadelta, Adagrad, Adamax, Nadam, RMSprop\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "sgd = SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "adad = Adadelta(lr=1.0,rho=0.95,epsilon=None,decay=0.0)\n",
    "adag = Adagrad(lr=0.01,epsilon=None,decay=0.0)\n",
    "adamax = Adamax(lr=0.002,beta_1=0.9,beta_2=0.999,epsilon=None,decay=0.0)\n",
    "nadam = Nadam(lr=0.002,beta_1=0.9,beta_2=0.999,epsilon=None,schedule_decay=0.004)\n",
    "rms = RMSprop(lr=0.001,rho=0.9,epsilon=None,decay=0.0)\n",
    "\n",
    "# Losses    https://keras.io/losses/\n",
    "loss = ['sparse_categorical_crossentropy','mean_squared_error','mean_absolute_error',\n",
    "        'categorical_crossentropy','categorical_hinge']\n",
    "\n",
    "# Metrics    https://www.tensorflow.org/api_docs/python/tf/metrics\n",
    "metrics = ['accuracy','precision','recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNQcFWhqaY2Z"
   },
   "outputs": [],
   "source": [
    "# Compile the model you created before using \n",
    "# adam optimizer as optimizer\n",
    "# sparse categorical crossentropy as loss function\n",
    "# accuracy as metric\n",
    "model.compile(optimizer=adam,\n",
    "              loss=loss[0],\n",
    "              metrics=[metrics[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9SvR02wtaY2c"
   },
   "source": [
    "## 1.2.3 Train the model \n",
    "The batch size is a number of samples processed before the model is updated.<br>\n",
    "The number of epochs is the number of complete passes through the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 93396,
     "status": "ok",
     "timestamp": 1588274268471,
     "user": {
      "displayName": "Nicoletta Noceti",
      "photoUrl": "",
      "userId": "00427744574778931345"
     },
     "user_tz": -120
    },
    "id": "Wd4Q9bFjaY2c",
    "outputId": "2fc1be16-0d2d-489e-9034-40ab0191b09d"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "# fit your model and save the returned value as \"history\". \n",
    "# Use both the train and validation set \n",
    "# Set both properly the batch size value and the epochs value\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, validation_data =(x_val, y_val), epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dv73zOpQaY2f"
   },
   "source": [
    "#### Training history visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuYCcmXGaY2h"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2177,
     "status": "ok",
     "timestamp": 1588274661606,
     "user": {
      "displayName": "Nicoletta Noceti",
      "photoUrl": "",
      "userId": "00427744574778931345"
     },
     "user_tz": -120
    },
    "id": "nXWlXkuBaY2t",
    "outputId": "b8687c88-d55d-4570-f0b4-e509bdb903d1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqEKGdT6aY2v"
   },
   "source": [
    "**What could you notice in the loss graph training the model over large number of epochs?**\n",
    "\n",
    "(Training loss continue to dicrease in a flatten way until to go near 0; validation loss after a while starts to increase significantly -> OVERFITTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8zJr0nraY2w"
   },
   "source": [
    "## 1.2.4 Evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7075,
     "status": "ok",
     "timestamp": 1588274670973,
     "user": {
      "displayName": "Nicoletta Noceti",
      "photoUrl": "",
      "userId": "00427744574778931345"
     },
     "user_tz": -120
    },
    "id": "2lxASSVmaY2x",
    "outputId": "5dd90fc5-441b-4472-bc01-c38a2f843e6e"
   },
   "outputs": [],
   "source": [
    "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2stTI0NaY2z"
   },
   "source": [
    "**Try to play with these parameters (loss, learning rate and optimizers) in order to see how this choice affects the accuracy.**\n",
    "What do you expect? which is faster?\n",
    "\n",
    "The learning rate can be passed as a parameter for the optimizer (e.g., tensorflow.keras.optimizers.Adam(lr=0.01)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "znWL73IWaY20"
   },
   "source": [
    "# 1.3 Overfitting\n",
    "Given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simpler models are less likely to overfit than complex ones.<br>\n",
    "A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether).<br>\n",
    "How to improve generalization of our model on unseen data?<br>\n",
    "In this section we will:\n",
    "* 1.3.1 Add weight regularization\n",
    "* 1.3.2 Dropout\n",
    "* 1.3.3 Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aimRiLIqaY22"
   },
   "source": [
    "## 1.3.1 Add weight regularization\n",
    "A common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more \"regular\".<br>\n",
    "This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights.<br> This cost comes in two flavors:\n",
    "* L1 regularization\n",
    "* L2 regularization\n",
    "\n",
    "In tf.keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20126,
     "status": "ok",
     "timestamp": 1588274703081,
     "user": {
      "displayName": "Nicoletta Noceti",
      "photoUrl": "",
      "userId": "00427744574778931345"
     },
     "user_tz": -120
    },
    "id": "vE2zlBgeaY23",
    "outputId": "407d8374-7841-4c1b-ee84-cddabeb6f85a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from  tensorflow.keras import regularizers\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "# Add a Dense layer with 512 neurons, with relu as activation function and a l2 regularizer (with 0.001 as regularization value) as kernel regularizer \n",
    "model.add(Dense(512, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "# Add a Dense layer with 256 neurons, with relu as activation function and a l2 regularizer (with 0.001 as regularization value) as kernel regularizer \n",
    "model.add(Dense(256, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "# Add a Dense layer with 128 neurons, with relu as activation function and a l2 regularizer (with 0.001 as regularization value) as kernel regularizer \n",
    "model.add(Dense(128, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "# Add a Dense layer with 64 neurons, with relu as activation function and a l2 regularizer (with 0.001 as regularization value) as kernel regularizer \n",
    "model.add(Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "# Add a Dense layer with number of neurons equal to the number of classes, with softmax as activation function\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model you just created using \n",
    "# adam optimizer as optimizer\n",
    "# sparse categorical crossentropy as loss function\n",
    "# accuracy as metric\n",
    "model.compile(optimizer=adam,\n",
    "              loss=loss[0],\n",
    "              metrics=[metrics[0]])\n",
    "\n",
    "# fit your model and save the returned value as \"history\". \n",
    "# Use both the train and validation set \n",
    "# Set both properly the batch size value and the epochs value\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data =(x_val, y_val))\n",
    "\n",
    "# Call the plot_history function to plot the obtained results\n",
    "plot_history(history)\n",
    "\n",
    "# Evaluate\n",
    "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r4xF5NBSaY25"
   },
   "source": [
    "## 1.3.2 Dropout\n",
    "Dropout is one of the most effective and most commonly used regularization techniques for neural networks.<br>\n",
    "Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. set to zero) a number of output features of the layer during training.<br>\n",
    "<img src=\"https://drive.google.com/uc?id=1PcGnn9cdM5JQk2nxRHAFPZAtBj1fNMvk\" width=\"600px\"><br>\n",
    "\n",
    "The \"dropout rate\" is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5; at test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17676,
     "status": "ok",
     "timestamp": 1588274731442,
     "user": {
      "displayName": "Nicoletta Noceti",
      "photoUrl": "",
      "userId": "00427744574778931345"
     },
     "user_tz": -120
    },
    "id": "EzV7zpjJaY26",
    "outputId": "5e1493c7-99e8-4079-b381-28bfb39b6f63",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "# Add a Dense layer with 512 neurons, with relu as activation function\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# Add a Dropout layer with 0.3 drop probability\n",
    "model.add(Dropout(0.3))\n",
    "# Add a Dense layer with 256 neurons, with relu as activation function\n",
    "model.add(Dense(256, activation='relu'))\n",
    "# Add a Dropout layer with 0.3 drop probability\n",
    "model.add(Dropout(0.3))\n",
    "# Add a Dense layer with 128 neurons, with relu as activation function\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Add a Dropout layer with 0.3 drop probability\n",
    "model.add(Dropout(0.3))\n",
    "# Add a Dense layer with 64 neurons, with relu as activation function\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# Add a Dense layer with number of neurons equal to the number of classes, with softmax as activation function\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model you just created using \n",
    "# adam optimizer as optimizer\n",
    "# sparse categorical crossentropy as loss function\n",
    "# accuracy as metric\n",
    "model.compile(optimizer=adam,\n",
    "              loss=loss[0],\n",
    "              metrics=[metrics[0]])\n",
    "\n",
    "# fit your model and save the returned value as \"history\". \n",
    "# Use both the train and validation set \n",
    "# Set both properly the batch size value and the epochs value\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data =(x_val, y_val))\n",
    "\n",
    "# Call the plot_history function to plot the obtained results\n",
    "plot_history(history)\n",
    "\n",
    "# Evaluate\n",
    "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5_Yf-4UaY29"
   },
   "source": [
    "## 1.3.3 Early stopping\n",
    "Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting.<br>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1prBt0w2oWIviRDjOSRKP2TglDGUr-EGZ\" width=\"400px\"><br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcJ9tjH2aY2-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "os.mkdir('my_checkpoint_dir')\n",
    "\n",
    "# early stopping https://keras.io/callbacks/\n",
    "es_callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "# Create checkpoint callback that will save the best model observed during training for later use\n",
    "checkpoint_path = \"my_checkpoint_dir/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = ModelCheckpoint(checkpoint_path,monitor='val_loss',save_weights_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26035,
     "status": "ok",
     "timestamp": 1588274863355,
     "user": {
      "displayName": "Nicoletta Noceti",
      "photoUrl": "",
      "userId": "00427744574778931345"
     },
     "user_tz": -120
    },
    "id": "oAaj7qisaY3B",
    "outputId": "1f282b24-4e3a-4850-983e-510cba34b84f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "# Add a Dense layer with 512 neurons, with relu as activation function\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# Add a Dense layer with 256 neurons, with relu as activation function\n",
    "model.add(Dense(256, activation='relu'))\n",
    "# Add a Dense layer with 128 neurons, with relu as activation function\n",
    "model.add(Dhttp://localhost:8888/notebooks/DNN_hands-on.ipynb#ense(128, activation='relu'))\n",
    "# Add a Dense layer with 64 neurons, with relu as activation function\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model you just created using \n",
    "# adam optimizer as optimizer\n",
    "# sparse categorical crossentropy as loss function\n",
    "# accuracy as metric\n",
    "model.compile(optimizer=adam,\n",
    "              loss=loss[0],\n",
    "              metrics=[metrics[0]])\n",
    "\n",
    "# fit your model and save the returned value as \"history\". \n",
    "# Use both the train and validation set \n",
    "# Set both properly the batch size value and the epochs value\n",
    "# Be careful to also set properly the callbacks parameter list\n",
    "batch_size =128\n",
    "epochs = 10\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, validation_data=(x_val, y_val), epochs=epochs, verbose=1, callbacks=[es_callback, cp_callback])\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "\n",
    "# Call the plot_history function to plot the obtained results\n",
    "plot_history(history)\n",
    "\n",
    "# Evaluate\n",
    "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MLP_DNN_complete.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
